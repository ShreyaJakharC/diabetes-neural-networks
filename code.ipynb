{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f990a437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.701358524956705\n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "diabetes_data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "\n",
    "X = diabetes_data.drop('Diabetes', axis=1)\n",
    "y = diabetes_data['Diabetes']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "perceptron = Perceptron(tol=1e-3, random_state=42)\n",
    "perceptron.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_scores = expit(perceptron.decision_function(X_test_scaled))\n",
    "auc_score = roc_auc_score(y_test, y_scores)\n",
    "\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29b81ec9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC scores by configuration:\n",
      "Layers: 1, Activation: identity, AUC: 0.824\n",
      "Layers: 1, Activation: relu, AUC: 0.832\n",
      "Layers: 1, Activation: logistic, AUC: 0.834\n",
      "Layers: 2, Activation: identity, AUC: 0.825\n",
      "Layers: 2, Activation: relu, AUC: 0.820\n",
      "Layers: 2, Activation: logistic, AUC: 0.833\n",
      "Layers: 3, Activation: identity, AUC: 0.825\n",
      "Layers: 3, Activation: relu, AUC: 0.801\n",
      "Layers: 3, Activation: logistic, AUC: 0.832\n"
     ]
    }
   ],
   "source": [
    "# Question 2\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "X = data.drop('Diabetes', axis=1) \n",
    "y = data['Diabetes']\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "configurations = [\n",
    "    (1, 'identity'),   \n",
    "    (1, 'relu'),       \n",
    "    (1, 'logistic'),   \n",
    "    (2, 'identity'),   \n",
    "    (2, 'relu'),       \n",
    "    (2, 'logistic'),   \n",
    "    (3, 'identity'),  \n",
    "    (3, 'relu'),      \n",
    "    (3, 'logistic')    \n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for layers, activation in configurations:\n",
    " \n",
    "    hidden_layer_sizes = tuple([50] * layers)  \n",
    "    mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, activation=activation, max_iter=500, random_state=42)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    \n",
    "   \n",
    "    if activation == 'identity':  \n",
    "        try:\n",
    "            y_scores = mlp.decision_function(X_test)\n",
    "        except AttributeError:\n",
    "            y_scores = mlp.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_scores = mlp.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    auc = roc_auc_score(y_test, y_scores)\n",
    "    results[(layers, activation)] = auc\n",
    "\n",
    "print(\"AUC scores by configuration:\")\n",
    "for config, auc in results.items():\n",
    "    print(f\"Layers: {config[0]}, Activation: {config[1]}, AUC: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "440acee2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khushiagarwal/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 320us/step - accuracy: 0.8614 - loss: 0.3256\n",
      "Epoch 2/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 314us/step - accuracy: 0.8665 - loss: 0.3164\n",
      "Epoch 3/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 314us/step - accuracy: 0.8675 - loss: 0.3132\n",
      "Epoch 4/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 315us/step - accuracy: 0.8673 - loss: 0.3131\n",
      "Epoch 5/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 318us/step - accuracy: 0.8671 - loss: 0.3126\n",
      "Epoch 6/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 315us/step - accuracy: 0.8683 - loss: 0.3115\n",
      "Epoch 7/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 323us/step - accuracy: 0.8679 - loss: 0.3130\n",
      "Epoch 8/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 320us/step - accuracy: 0.8672 - loss: 0.3144\n",
      "Epoch 9/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 318us/step - accuracy: 0.8672 - loss: 0.3129\n",
      "Epoch 10/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 320us/step - accuracy: 0.8684 - loss: 0.3108\n",
      "Epoch 11/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 316us/step - accuracy: 0.8668 - loss: 0.3123\n",
      "Epoch 12/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 317us/step - accuracy: 0.8693 - loss: 0.3110\n",
      "Epoch 13/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 322us/step - accuracy: 0.8694 - loss: 0.3100\n",
      "Epoch 14/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 319us/step - accuracy: 0.8702 - loss: 0.3094\n",
      "Epoch 15/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 315us/step - accuracy: 0.8698 - loss: 0.3105\n",
      "Epoch 16/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 317us/step - accuracy: 0.8694 - loss: 0.3086\n",
      "Epoch 17/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 316us/step - accuracy: 0.8686 - loss: 0.3113\n",
      "Epoch 18/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 316us/step - accuracy: 0.8695 - loss: 0.3111\n",
      "Epoch 19/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 319us/step - accuracy: 0.8706 - loss: 0.3081\n",
      "Epoch 20/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 318us/step - accuracy: 0.8702 - loss: 0.3103\n",
      "Epoch 21/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 322us/step - accuracy: 0.8692 - loss: 0.3098\n",
      "Epoch 22/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 317us/step - accuracy: 0.8706 - loss: 0.3098\n",
      "Epoch 23/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 318us/step - accuracy: 0.8711 - loss: 0.3079\n",
      "Epoch 24/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 317us/step - accuracy: 0.8712 - loss: 0.3069\n",
      "Epoch 25/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 332us/step - accuracy: 0.8705 - loss: 0.3084\n",
      "Epoch 26/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 323us/step - accuracy: 0.8691 - loss: 0.3092\n",
      "Epoch 27/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 319us/step - accuracy: 0.8688 - loss: 0.3105\n",
      "Epoch 28/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 319us/step - accuracy: 0.8688 - loss: 0.3097\n",
      "Epoch 29/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 315us/step - accuracy: 0.8703 - loss: 0.3087\n",
      "Epoch 30/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 317us/step - accuracy: 0.8711 - loss: 0.3064\n",
      "Epoch 31/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 320us/step - accuracy: 0.8703 - loss: 0.3090\n",
      "Epoch 32/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 318us/step - accuracy: 0.8714 - loss: 0.3061\n",
      "Epoch 33/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 323us/step - accuracy: 0.8703 - loss: 0.3077\n",
      "Epoch 34/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 317us/step - accuracy: 0.8707 - loss: 0.3067\n",
      "Epoch 35/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 318us/step - accuracy: 0.8718 - loss: 0.3051\n",
      "Epoch 36/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 320us/step - accuracy: 0.8696 - loss: 0.3076\n",
      "Epoch 37/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 322us/step - accuracy: 0.8714 - loss: 0.3048\n",
      "Epoch 38/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 328us/step - accuracy: 0.8711 - loss: 0.3068\n",
      "Epoch 39/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 330us/step - accuracy: 0.8701 - loss: 0.3083\n",
      "Epoch 40/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 330us/step - accuracy: 0.8696 - loss: 0.3081\n",
      "Epoch 41/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 335us/step - accuracy: 0.8712 - loss: 0.3061\n",
      "Epoch 42/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 328us/step - accuracy: 0.8709 - loss: 0.3073\n",
      "Epoch 43/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 333us/step - accuracy: 0.8714 - loss: 0.3073\n",
      "Epoch 44/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 333us/step - accuracy: 0.8723 - loss: 0.3049\n",
      "Epoch 45/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 329us/step - accuracy: 0.8709 - loss: 0.3068\n",
      "Epoch 46/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 334us/step - accuracy: 0.8713 - loss: 0.3059\n",
      "Epoch 47/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 334us/step - accuracy: 0.8717 - loss: 0.3067\n",
      "Epoch 48/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 332us/step - accuracy: 0.8714 - loss: 0.3056\n",
      "Epoch 49/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 333us/step - accuracy: 0.8712 - loss: 0.3060\n",
      "Epoch 50/50\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 329us/step - accuracy: 0.8719 - loss: 0.3061\n",
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197us/step\n",
      "The AUC score of the deep network is: 0.8261782068496183\n"
     ]
    }
   ],
   "source": [
    "# Question 3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "X = data.drop('Diabetes', axis=1)\n",
    "y = data['Diabetes']\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),  \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=1)\n",
    "\n",
    "\n",
    "y_pred_proba = model.predict(X_test).ravel()\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f'The AUC score of the deep network is: {auc_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d114ba11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with identity activation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khushiagarwal/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175us/step\n",
      "Activation: identity, RMSE: 6.1389578283174675\n",
      "Training with logistic activation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khushiagarwal/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175us/step\n",
      "Activation: logistic, RMSE: 5.990831278213405\n",
      "Training with relu activation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khushiagarwal/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170us/step\n",
      "Activation: relu, RMSE: 6.010831566111916\n",
      "Training with tanh activation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khushiagarwal/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173us/step\n",
      "Activation: tanh, RMSE: 6.00617925214004\n",
      "Activation: identity, RMSE: 6.1390\n",
      "Activation: logistic, RMSE: 5.9908\n",
      "Activation: relu, RMSE: 6.0108\n",
      "Activation: tanh, RMSE: 6.0062\n"
     ]
    }
   ],
   "source": [
    "# Question 4\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "X = data.drop('BMI', axis=1)  \n",
    "y = data['BMI']  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "def create_model(activation_function):\n",
    "    \n",
    "    if activation_function == 'identity':\n",
    "        activation_function = None  \n",
    "    elif activation_function == 'logistic':\n",
    "        activation_function = 'sigmoid'  \n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(64, input_shape=(X_train_scaled.shape[1],), activation=activation_function),\n",
    "        Dense(1)  \n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "activations = ['identity', 'logistic', 'relu', 'tanh'] \n",
    "\n",
    "\n",
    "results = []\n",
    "for activation in activations:\n",
    "    print(f\"Training with {activation} activation...\")\n",
    "    model = create_model(activation)\n",
    "    model.fit(X_train_scaled, y_train, epochs=50, batch_size=10, verbose=0)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    results.append((activation, rmse))\n",
    "    print(f\"Activation: {activation}, RMSE: {rmse}\")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Activation: {result[0]}, RMSE: {result[1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21b21349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khushiagarwal/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 83.7034 - val_loss: 73.0205\n",
      "Epoch 2/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 367us/step - loss: 40.3507 - val_loss: 71.1038\n",
      "Epoch 3/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 37.5056 - val_loss: 55.5946\n",
      "Epoch 4/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 36.6519 - val_loss: 52.0254\n",
      "Epoch 5/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 370us/step - loss: 37.0007 - val_loss: 48.0620\n",
      "Epoch 6/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 364us/step - loss: 36.5506 - val_loss: 43.1296\n",
      "Epoch 7/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 362us/step - loss: 36.3429 - val_loss: 44.7275\n",
      "Epoch 8/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 370us/step - loss: 36.5806 - val_loss: 45.7097\n",
      "Epoch 9/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 364us/step - loss: 36.3315 - val_loss: 41.9874\n",
      "Epoch 10/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 364us/step - loss: 37.1725 - val_loss: 45.2605\n",
      "Epoch 11/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 364us/step - loss: 36.9856 - val_loss: 43.9782\n",
      "Epoch 12/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 367us/step - loss: 36.3618 - val_loss: 42.7029\n",
      "Epoch 13/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 365us/step - loss: 36.6043 - val_loss: 40.3291\n",
      "Epoch 14/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 364us/step - loss: 36.3773 - val_loss: 43.0513\n",
      "Epoch 15/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 365us/step - loss: 37.0131 - val_loss: 41.1630\n",
      "Epoch 16/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 369us/step - loss: 35.7132 - val_loss: 39.6885\n",
      "Epoch 17/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 368us/step - loss: 35.5813 - val_loss: 41.3597\n",
      "Epoch 18/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 37.3684 - val_loss: 39.8201\n",
      "Epoch 19/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 370us/step - loss: 36.1649 - val_loss: 41.1445\n",
      "Epoch 20/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 365us/step - loss: 36.0549 - val_loss: 43.1235\n",
      "Epoch 21/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 371us/step - loss: 35.9180 - val_loss: 41.0848\n",
      "Epoch 22/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 368us/step - loss: 36.3092 - val_loss: 40.6065\n",
      "Epoch 23/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 367us/step - loss: 35.4914 - val_loss: 43.1043\n",
      "Epoch 24/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 370us/step - loss: 36.1404 - val_loss: 42.1513\n",
      "Epoch 25/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 367us/step - loss: 35.8850 - val_loss: 42.0740\n",
      "Epoch 26/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 365us/step - loss: 36.6412 - val_loss: 41.1227\n",
      "Epoch 27/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 369us/step - loss: 35.6606 - val_loss: 39.1365\n",
      "Epoch 28/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 369us/step - loss: 35.9381 - val_loss: 39.7312\n",
      "Epoch 29/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 370us/step - loss: 36.0242 - val_loss: 39.8059\n",
      "Epoch 30/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 371us/step - loss: 36.0035 - val_loss: 41.6190\n",
      "Epoch 31/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 369us/step - loss: 35.8080 - val_loss: 40.9653\n",
      "Epoch 32/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 367us/step - loss: 35.7607 - val_loss: 40.1440\n",
      "Epoch 33/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 369us/step - loss: 36.1720 - val_loss: 40.0703\n",
      "Epoch 34/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 36.8200 - val_loss: 42.1461\n",
      "Epoch 35/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 370us/step - loss: 35.7366 - val_loss: 40.8250\n",
      "Epoch 36/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 367us/step - loss: 36.1953 - val_loss: 41.1750\n",
      "Epoch 37/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 368us/step - loss: 35.5714 - val_loss: 39.8206\n",
      "Epoch 38/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35.2871 - val_loss: 39.0730\n",
      "Epoch 39/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 370us/step - loss: 35.3710 - val_loss: 40.1057\n",
      "Epoch 40/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 372us/step - loss: 36.1443 - val_loss: 40.1736\n",
      "Epoch 41/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 370us/step - loss: 35.8279 - val_loss: 40.0063\n",
      "Epoch 42/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 370us/step - loss: 35.9532 - val_loss: 41.8445\n",
      "Epoch 43/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 370us/step - loss: 36.4759 - val_loss: 41.0813\n",
      "Epoch 44/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 369us/step - loss: 35.3867 - val_loss: 39.9761\n",
      "Epoch 45/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 368us/step - loss: 35.2785 - val_loss: 40.8719\n",
      "Epoch 46/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 370us/step - loss: 36.0333 - val_loss: 40.5858\n",
      "Epoch 47/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 372us/step - loss: 35.8752 - val_loss: 39.9155\n",
      "Epoch 48/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 370us/step - loss: 35.5234 - val_loss: 39.1704\n",
      "Epoch 49/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 36.1479 - val_loss: 40.3799\n",
      "Epoch 50/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 36.1216 - val_loss: 40.2371\n",
      "Epoch 51/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 372us/step - loss: 35.4083 - val_loss: 40.2589\n",
      "Epoch 52/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35.4852 - val_loss: 41.0314\n",
      "Epoch 53/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 35.8224 - val_loss: 40.3528\n",
      "Epoch 54/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 35.4766 - val_loss: 42.5359\n",
      "Epoch 55/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 36.0473 - val_loss: 41.5647\n",
      "Epoch 56/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 368us/step - loss: 35.7972 - val_loss: 39.9856\n",
      "Epoch 57/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35.0643 - val_loss: 41.3204\n",
      "Epoch 58/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 371us/step - loss: 35.2998 - val_loss: 40.2390\n",
      "Epoch 59/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 371us/step - loss: 35.7613 - val_loss: 40.4748\n",
      "Epoch 60/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 370us/step - loss: 35.5033 - val_loss: 40.4837\n",
      "Epoch 61/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 371us/step - loss: 35.4786 - val_loss: 39.6403\n",
      "Epoch 62/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35.2591 - val_loss: 40.9963\n",
      "Epoch 63/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 36.0028 - val_loss: 41.2094\n",
      "Epoch 64/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 35.4770 - val_loss: 39.3534\n",
      "Epoch 65/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 372us/step - loss: 36.1364 - val_loss: 41.5510\n",
      "Epoch 66/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 370us/step - loss: 35.4820 - val_loss: 40.7259\n",
      "Epoch 67/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 35.1553 - val_loss: 40.5545\n",
      "Epoch 68/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 35.0091 - val_loss: 40.3688\n",
      "Epoch 69/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 35.6633 - val_loss: 40.7229\n",
      "Epoch 70/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35.7461 - val_loss: 41.2003\n",
      "Epoch 71/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35.7988 - val_loss: 39.7089\n",
      "Epoch 72/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 34.8680 - val_loss: 39.8407\n",
      "Epoch 73/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35.4584 - val_loss: 39.8834\n",
      "Epoch 74/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 36.3047 - val_loss: 40.3547\n",
      "Epoch 75/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 371us/step - loss: 35.6325 - val_loss: 40.7034\n",
      "Epoch 76/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 36.0879 - val_loss: 40.6357\n",
      "Epoch 77/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 35.6235 - val_loss: 40.8992\n",
      "Epoch 78/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35.8496 - val_loss: 39.5282\n",
      "Epoch 79/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35.5011 - val_loss: 41.7939\n",
      "Epoch 80/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 372us/step - loss: 35.4734 - val_loss: 40.0076\n",
      "Epoch 81/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377us/step - loss: 35.7127 - val_loss: 39.8751\n",
      "Epoch 82/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 34.9105 - val_loss: 40.9851\n",
      "Epoch 83/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35.2426 - val_loss: 40.5719\n",
      "Epoch 84/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35.4003 - val_loss: 40.2217\n",
      "Epoch 85/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 372us/step - loss: 36.1894 - val_loss: 40.2359\n",
      "Epoch 86/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 34.9666 - val_loss: 40.5626\n",
      "Epoch 87/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35.7773 - val_loss: 40.6965\n",
      "Epoch 88/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375us/step - loss: 35.3125 - val_loss: 40.7797\n",
      "Epoch 89/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35.4665 - val_loss: 41.1962\n",
      "Epoch 90/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 373us/step - loss: 35.2164 - val_loss: 39.9790\n",
      "Epoch 91/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 35.2935 - val_loss: 39.8185\n",
      "Epoch 92/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 35.4641 - val_loss: 42.0893\n",
      "Epoch 93/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379us/step - loss: 35.3781 - val_loss: 40.0869\n",
      "Epoch 94/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 381us/step - loss: 35.4497 - val_loss: 41.5775\n",
      "Epoch 95/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 35.4697 - val_loss: 42.3977\n",
      "Epoch 96/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - loss: 35.5452 - val_loss: 40.2378\n",
      "Epoch 97/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374us/step - loss: 36.0589 - val_loss: 41.2215\n",
      "Epoch 98/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383us/step - loss: 35.7846 - val_loss: 39.0838\n",
      "Epoch 99/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 35.2183 - val_loss: 39.4326\n",
      "Epoch 100/100\n",
      "\u001b[1m5550/5550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380us/step - loss: 35.7276 - val_loss: 40.0510\n",
      "\u001b[1m1190/1190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200us/step\n",
      "Test RMSE: 6.139734245323259\n"
     ]
    }
   ],
   "source": [
    "# Question 5\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "X = data.drop('BMI', axis=1)  \n",
    "y = data['BMI']\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Test RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "762ba73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikeras\n",
      "  Obtaining dependency information for scikeras from https://files.pythonhosted.org/packages/ea/09/1c02aa24daf7a003c06f629fbb69dc9ae1bda1b247d7b8981e550d752ac9/scikeras-0.13.0-py3-none-any.whl.metadata\n",
      "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: keras>=3.2.0 in ./anaconda3/lib/python3.11/site-packages (from scikeras) (3.2.1)\n",
      "Collecting scikit-learn>=1.4.2 (from scikeras)\n",
      "  Obtaining dependency information for scikit-learn>=1.4.2 from https://files.pythonhosted.org/packages/f2/30/1299e84d2ba3bc735baf17cebbf5b9d55144243c41b3ec6559ce3cf61e23/scikit_learn-1.4.2-cp311-cp311-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading scikit_learn-1.4.2-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: absl-py in ./anaconda3/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (2.1.0)\n",
      "Requirement already satisfied: numpy in ./anaconda3/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (1.24.3)\n",
      "Requirement already satisfied: rich in ./anaconda3/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (13.7.1)\n",
      "Requirement already satisfied: namex in ./anaconda3/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
      "Requirement already satisfied: h5py in ./anaconda3/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (3.11.0)\n",
      "Requirement already satisfied: optree in ./anaconda3/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in ./anaconda3/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (0.3.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.4.2->scikeras) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.4.2->scikeras) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.4.2->scikeras) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./anaconda3/lib/python3.11/site-packages (from optree->keras>=3.2.0->scikeras) (4.7.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./anaconda3/lib/python3.11/site-packages (from rich->keras>=3.2.0->scikeras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./anaconda3/lib/python3.11/site-packages (from rich->keras>=3.2.0->scikeras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.0)\n",
      "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
      "Downloading scikit_learn-1.4.2-cp311-cp311-macosx_12_0_arm64.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scikit-learn, scikeras\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.3.0\n",
      "    Uninstalling scikit-learn-1.3.0:\n",
      "      Successfully uninstalled scikit-learn-1.3.0\n",
      "Successfully installed scikeras-0.13.0 scikit-learn-1.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2893775a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m    1/20295\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m57:49\u001b[0m 171ms/step - loss: 816.5577"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khushiagarwal/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 194us/step - loss: 88.5131\n",
      "Epoch 2/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 196us/step - loss: 36.7672\n",
      "Epoch 3/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 199us/step - loss: 35.9431\n",
      "Epoch 4/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 193us/step - loss: 36.1352\n",
      "Epoch 5/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 191us/step - loss: 36.3920\n",
      "Epoch 6/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 193us/step - loss: 36.1288\n",
      "Epoch 7/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 193us/step - loss: 36.3370\n",
      "Epoch 8/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 194us/step - loss: 36.0112\n",
      "Epoch 9/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 194us/step - loss: 35.2420\n",
      "Epoch 10/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 192us/step - loss: 36.0186\n",
      "Epoch 11/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 193us/step - loss: 36.1814\n",
      "Epoch 12/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 192us/step - loss: 36.3397\n",
      "Epoch 13/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 193us/step - loss: 36.8448\n",
      "Epoch 14/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 193us/step - loss: 36.4831\n",
      "Epoch 15/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 193us/step - loss: 36.3132\n",
      "Epoch 16/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 193us/step - loss: 36.0744\n",
      "Epoch 17/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 195us/step - loss: 36.3239\n",
      "Epoch 18/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 194us/step - loss: 35.6513\n",
      "Epoch 19/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 192us/step - loss: 35.6231\n",
      "Epoch 20/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 195us/step - loss: 35.7696\n",
      "Epoch 21/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 198us/step - loss: 35.4733\n",
      "Epoch 22/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 192us/step - loss: 35.8969\n",
      "Epoch 23/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 194us/step - loss: 36.4259\n",
      "Epoch 24/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 193us/step - loss: 35.8636\n",
      "Epoch 25/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 192us/step - loss: 36.2148\n",
      "Epoch 26/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 196us/step - loss: 35.5565\n",
      "Epoch 27/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 196us/step - loss: 35.8406\n",
      "Epoch 28/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 196us/step - loss: 35.9991\n",
      "Epoch 29/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 196us/step - loss: 36.1098\n",
      "Epoch 30/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 196us/step - loss: 35.4013\n",
      "Epoch 31/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 195us/step - loss: 35.7466\n",
      "Epoch 32/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 196us/step - loss: 37.0753\n",
      "Epoch 33/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 196us/step - loss: 35.8239\n",
      "Epoch 34/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 198us/step - loss: 36.1807\n",
      "Epoch 35/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 195us/step - loss: 36.1130\n",
      "Epoch 36/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 199us/step - loss: 35.9082\n",
      "Epoch 37/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 202us/step - loss: 35.8712\n",
      "Epoch 38/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 202us/step - loss: 35.7802\n",
      "Epoch 39/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 197us/step - loss: 35.5468\n",
      "Epoch 40/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 199us/step - loss: 35.7376\n",
      "Epoch 41/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 197us/step - loss: 35.9390\n",
      "Epoch 42/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 196us/step - loss: 36.2475\n",
      "Epoch 43/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 197us/step - loss: 35.7014\n",
      "Epoch 44/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 202us/step - loss: 35.8482\n",
      "Epoch 45/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 200us/step - loss: 35.6525\n",
      "Epoch 46/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 200us/step - loss: 35.6818\n",
      "Epoch 47/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 200us/step - loss: 35.9195\n",
      "Epoch 48/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 200us/step - loss: 36.5127\n",
      "Epoch 49/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 199us/step - loss: 35.7490\n",
      "Epoch 50/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 205us/step - loss: 35.8656\n",
      "Epoch 51/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 218us/step - loss: 35.3162\n",
      "Epoch 52/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 204us/step - loss: 35.6502\n",
      "Epoch 53/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 204us/step - loss: 36.3937\n",
      "Epoch 54/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 200us/step - loss: 35.7729\n",
      "Epoch 55/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 202us/step - loss: 35.5612\n",
      "Epoch 56/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 200us/step - loss: 35.8664\n",
      "Epoch 57/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 203us/step - loss: 35.9854\n",
      "Epoch 58/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 204us/step - loss: 35.4908\n",
      "Epoch 59/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 198us/step - loss: 35.3942\n",
      "Epoch 60/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 201us/step - loss: 35.7718\n",
      "Epoch 61/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 204us/step - loss: 36.2631\n",
      "Epoch 62/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 204us/step - loss: 35.5667\n",
      "Epoch 63/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 202us/step - loss: 35.9822\n",
      "Epoch 64/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 200us/step - loss: 35.6175\n",
      "Epoch 65/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 206us/step - loss: 35.4727\n",
      "Epoch 66/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 200us/step - loss: 35.8539\n",
      "Epoch 67/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 203us/step - loss: 36.0830\n",
      "Epoch 68/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 198us/step - loss: 36.1707\n",
      "Epoch 69/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 198us/step - loss: 34.7922\n",
      "Epoch 70/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 204us/step - loss: 35.8855\n",
      "Epoch 71/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 206us/step - loss: 35.5419\n",
      "Epoch 72/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 205us/step - loss: 35.4981\n",
      "Epoch 73/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 207us/step - loss: 35.9049\n",
      "Epoch 74/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 199us/step - loss: 35.9372\n",
      "Epoch 75/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 200us/step - loss: 35.5608\n",
      "Epoch 76/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 197us/step - loss: 35.8149\n",
      "Epoch 77/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 200us/step - loss: 35.6950\n",
      "Epoch 78/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 198us/step - loss: 36.5294\n",
      "Epoch 79/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 201us/step - loss: 35.6126\n",
      "Epoch 80/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 195us/step - loss: 35.9427\n",
      "Epoch 81/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 208us/step - loss: 35.3553\n",
      "Epoch 82/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 196us/step - loss: 35.8775\n",
      "Epoch 83/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 195us/step - loss: 36.0016\n",
      "Epoch 84/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 195us/step - loss: 35.8113\n",
      "Epoch 85/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 196us/step - loss: 35.2858\n",
      "Epoch 86/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 196us/step - loss: 35.7765\n",
      "Epoch 87/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 196us/step - loss: 36.1835\n",
      "Epoch 88/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 195us/step - loss: 36.0068\n",
      "Epoch 89/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 196us/step - loss: 35.2589\n",
      "Epoch 90/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 197us/step - loss: 35.7845\n",
      "Epoch 91/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 196us/step - loss: 35.8248\n",
      "Epoch 92/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 195us/step - loss: 35.7622\n",
      "Epoch 93/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 196us/step - loss: 35.9419\n",
      "Epoch 94/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 196us/step - loss: 35.7524\n",
      "Epoch 95/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 200us/step - loss: 35.2463\n",
      "Epoch 96/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 196us/step - loss: 35.9795\n",
      "Epoch 97/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 196us/step - loss: 35.9082\n",
      "Epoch 98/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 197us/step - loss: 35.9439\n",
      "Epoch 99/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 197us/step - loss: 35.4194\n",
      "Epoch 100/100\n",
      "\u001b[1m20295/20295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 197us/step - loss: 35.5857\n",
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178us/step\n",
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167us/step\n",
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167us/step\n",
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172us/step\n",
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168us/step\n",
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168us/step\n",
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173us/step\n",
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168us/step\n",
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168us/step\n",
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171us/step\n",
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169us/step\n",
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174us/step\n",
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168us/step\n",
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173us/step\n",
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168us/step\n",
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172us/step\n",
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168us/step\n",
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168us/step\n",
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173us/step\n",
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167us/step\n",
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174us/step\n",
      "\u001b[1m1586/1586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167us/step\n",
      "                  Feature  Importance\n",
      "10          HasHealthcare    0.001939\n",
      "20                 Zodiac    0.000417\n",
      "13           MentalHealth   -0.002862\n",
      "11  NotAbleToAffordDoctor   -0.003891\n",
      "7                   Fruit   -0.005243\n",
      "8              Vegetables   -0.005420\n",
      "4                  Stroke   -0.006825\n",
      "5              Myocardial   -0.007499\n",
      "14         PhysicalHealth   -0.012170\n",
      "2                HighChol   -0.018013\n",
      "9            HeavyDrinker   -0.018534\n",
      "18       EducationBracket   -0.034215\n",
      "6            PhysActivity   -0.034670\n",
      "19          IncomeBracket   -0.034935\n",
      "3                  Smoker   -0.046510\n",
      "16          BiologicalSex   -0.053450\n",
      "0                Diabetes   -0.122749\n",
      "1                  HighBP   -0.155473\n",
      "12          GeneralHealth   -0.168528\n",
      "15      HardToClimbStairs   -0.188418\n",
      "17             AgeBracket   -0.339584\n"
     ]
    }
   ],
   "source": [
    "# Extra Credit 1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "X = data.drop('BMI', axis=1) \n",
    "y = data['BMI']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=10, verbose=1)\n",
    "\n",
    "\n",
    "def calculate_rmse(predictions, targets):\n",
    "    return np.sqrt(mean_squared_error(predictions, targets))\n",
    "\n",
    "\n",
    "original_pred = model.predict(X_test_scaled)\n",
    "original_rmse = calculate_rmse(original_pred, y_test)\n",
    "\n",
    "\n",
    "importance_scores = {}\n",
    "for i, col in enumerate(X.columns):\n",
    "    X_test_permuted = X_test_scaled.copy()\n",
    "    np.random.shuffle(X_test_permuted[:, i])  \n",
    "    permuted_pred = model.predict(X_test_permuted)\n",
    "    permuted_rmse = calculate_rmse(permuted_pred, y_test)\n",
    "    importance_scores[col] = original_rmse - permuted_rmse  \n",
    "\n",
    "importance_df = pd.DataFrame(list(importance_scores.items()), columns=['Feature', 'Importance'])\n",
    "print(importance_df.sort_values(by='Importance', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69989b11",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pd_fillna' from 'sklearn.utils.fixes' (/Users/khushiagarwal/anaconda3/lib/python3.11/site-packages/sklearn/utils/fixes.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Extra Credit 2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_classification\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/datasets/__init__.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lfw\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_lfw_pairs, fetch_lfw_people\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_olivetti_faces\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_olivetti_faces\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_openml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_openml\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_rcv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_rcv1\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_samples_generator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     make_biclusters,\n\u001b[1;32m     30\u001b[0m     make_blobs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     make_swiss_roll,\n\u001b[1;32m     49\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/datasets/_openml.py:30\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     Integral,\n\u001b[1;32m     24\u001b[0m     Interval,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     validate_params,\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_data_home\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_arff_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_arff_from_gzip_file\n\u001b[1;32m     32\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfetch_openml\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     34\u001b[0m _OPENML_PREFIX \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.openml.org/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/datasets/_arff_parser.py:18\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternals\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_arff\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArffSparseDataType\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     _chunk_generator,\n\u001b[1;32m     15\u001b[0m     check_pandas_support,\n\u001b[1;32m     16\u001b[0m     get_chunk_n_rows,\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pd_fillna\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_split_sparse_columns\u001b[39m(\n\u001b[1;32m     22\u001b[0m     arff_data: ArffSparseDataType, include_columns: List\n\u001b[1;32m     23\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArffSparseDataType:\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Obtains several columns from sparse ARFF representation. Additionally,\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    the column indices are re-labelled, given the columns that are not\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    included. (e.g., when including [1, 2, 3], the columns will be relabelled\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m        include_columns argument.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'pd_fillna' from 'sklearn.utils.fixes' (/Users/khushiagarwal/anaconda3/lib/python3.11/site-packages/sklearn/utils/fixes.py)"
     ]
    }
   ],
   "source": [
    "# Extra Credit 2\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=0, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Neural Network\n",
    "nn_model = Sequential([\n",
    "    Dense(10, activation='relu', input_dim=20),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn_model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)\n",
    "\n",
    "# Other models\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "tree_model = DecisionTreeClassifier()\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "forest_model = RandomForestClassifier()\n",
    "forest_model.fit(X_train, y_train)\n",
    "\n",
    "gb_model = GradientBoostingClassifier()\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the models\n",
    "models = {'Neural Network': nn_model, 'Logistic Regression': lr_model, 'Decision Tree': tree_model, \n",
    "          'Random Forest': forest_model, 'Gradient Boosting': gb_model}\n",
    "for name, model in models.items():\n",
    "    if name == 'Neural Network':\n",
    "        loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(f'{name} Accuracy: {accuracy:.4f}')\n",
    "    else:\n",
    "        predictions = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        print(f'{name} Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6fc5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
